<!DOCTYPE html>
<html lang="en">

<style>
    /* images should be limited to  800px, centered, and with padding 20px*/

    img {
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: min(800px, 100% - 40px);
        padding: 20px;
    }

    /* center the body */
    body {
        font-family: 'Roboto', sans-serif;
        margin: 0 auto;
        /* Center the body horizontally */
        padding: 30px;
        box-sizing: border-box;
        line-height: 1.6;
        letter-spacing: 0.5px;
        max-width: max(60%, 600px);
    }

    .content-wrapper {
        /* max width of either 60% or 600px, whichever is larger */
        max-width: max(60%, 600px);
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        font-family: 'Space Mono', monospace;
        margin-bottom: 6px;
    }

    .title {
        font-size: 2.5em;
    }

    h1 {
        font-size: 2em;
    }

    h2 {
        font-size: 1.7em;
    }

    h3 {
        font-size: 1.6em;
    }

    h4 {
        font-size: 1.1em;
    }

    p {
        margin-bottom: 15px;
    }

    em {
        font-style: italic;
    }

    a {
        color: #e1b60e;
        /* Pastel yellow */
        text-decoration: underline;
        font-family: 'Space Mono', monospace;
    }

    u {
        text-decoration: underline;
    }

    strong {
        font-weight: bold;
    }

    .nav-links {
        display: flex;
        margin-bottom: 0px;
        width: 100%;
    }

    .clear {
        font-family: 'Space Mono', monospace;
        background-color: transparent;
        border-width: 0px;
        bottom: 30px;
        left: 30px;
        position: absolute;
        text-decoration: underline;
    }

    /* Brutalist Table Styling */
    table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 20px;
        font-family: 'Space Mono', monospace;
    }

    th,
    td {
        padding: 10px;
        text-align: left;
        border: 2px solid #000;
    }

    pre {
        padding: 10px;
        /* Add padding to all lines of the code block */
        /* Optional: background color for better visibility */

        /* Optional: border for better visibility */
        overflow: auto;
        /* Optional: add scrollbars if content overflows */
        /* Add black shadow with no blur */

    }

    ul {
        list-style: none;
        padding-left: 30px;
    }

    ul li {
        position: relative;
        padding-left: 30px;
        margin-bottom: 10px;
    }

    ul li::before {
        content: '■';
        position: absolute;
        left: 0;
        color: #000;
        font-size: 1.2em;
        font-weight: bold;
    }

    /* Footnotes */
    .footnotes {
        margin-top: 30px;
        padding-top: 10px;
        border-top: 1px solid #ddd;
        font-size: 0.9em;
    }

    .footnotes ol {
        padding-left: 20px;
    }

    .footnotes li {
        margin-bottom: 8px;
    }

    .footnote-definition {
        margin-bottom: 8px;
    }

    .footnote-definition p {
        display: inline;
    }

    .footnotes-separator {
        border: none;
        border-top: 1px solid #ddd;
        margin: 30px 0 15px 0;
    }

    .footnote-reference,
    .footnote-ref {
        font-size: 0.8em;
        vertical-align: super;
    }

    .footnote-reference a,
    .footnote-ref a,
    .footnote-backref {
        text-decoration: none;
    }



    /* Nested Lists */
    ul ul,
    ol ol,
    ul ol,
    ol ul {
        margin-top: 10px;
    }

    th {
        background-color: #000;
        color: #fff;
        font-weight: bold;
        text-transform: uppercase;
    }

    tr:nth-child(even) {
        background-color: #f0f0f0;
    }

    /* Responsive table */
    @media screen and (max-width: 600px) {

        table,
        thead,
        tbody,
        th,
        td,
        tr {
            display: block;
        }

        thead tr {
            position: absolute;
            top: -9999px;
            left: -9999px;
        }

        tr {
            margin-bottom: 10px;
            border: 2px solid #000;
        }

        td {
            border: none;
            border-bottom: 1px solid #000;
            position: relative;
            padding-left: 50%;
        }

        td:before {
            content: attr(data-label);
            position: absolute;
            left: 6px;
            width: 45%;
            padding-right: 10px;
            white-space: nowrap;
            font-weight: bold;
            text-transform: uppercase;
        }
    }
</style>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Redundant Attention Heads in Large Language Models For In Context Learning | Sudarsh Kunnavakkam</title>
    <meta name="description"
        content="Research on redundant attention heads in language models and their role in in-context learning through Bayesian updates.">
    <meta name="author" content="Sudarsh Kunnavakkam">
    <meta name="keywords" content="machine learning, AI safety, language models, nanophotonics, Caltech, research">

    <link rel="canonical" href="https://sudarsh.com/blog/degenerate-attention-heads/">

    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Space+Mono&display=swap"
        rel="stylesheet">
    <script data-goatcounter="https://skunnavakkam.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
        integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Redundant Attention Heads in Large Language Models For In Context Learning" />
    <meta name="twitter:description"
        content="Research on redundant attention heads in language models and their role in in-context learning through Bayesian updates." />
    <meta name="twitter:url" content="https://sudarsh.com" />
    

    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });

            const referenceNodes = document.getElementsByClassName('footnote-reference');
            const referenceLinks = referenceNodes.length
                ? Array.from(referenceNodes).map((node) => node.querySelector('a')).filter(Boolean)
                : Array.from(document.querySelectorAll('sup a[href^="#fn"]'));

            for (const link of referenceLinks) {
                const href = link.getAttribute('href');
                if (!href || !href.startsWith('#')) continue;
                const id = href.slice(1);
                if (!link.getAttribute('id')) {
                    link.setAttribute('id', `${id}_ref`);
                }
            }

            const definitionNodes = document.getElementsByClassName('footnote-definition');
            const definitionItems = definitionNodes.length
                ? Array.from(definitionNodes)
                : Array.from(document.querySelectorAll('.footnotes li[id]'));

            for (const footnote of definitionItems) {
                const id = footnote.getAttribute('id');
                if (!id || footnote.querySelector('.footnote-backref')) continue;
                const backReference = document.createElement('a');
                backReference.className = 'footnote-backref';
                backReference.setAttribute('href', `#${id}_ref`);
                backReference.textContent = '↩';
                footnote.append(backReference);
            }

            const definitionBlocks = document.querySelectorAll('.footnote-definition');
            for (const definition of definitionBlocks) {
                const label = definition.querySelector('.footnote-definition-label');
                const paragraph = definition.querySelector('p');
                if (!label || !paragraph) continue;
                if (!paragraph.contains(label)) {
                    paragraph.prepend(label, ' ');
                }
            }

            const existingSeparator = document.querySelector('.footnotes-separator');
            if (!existingSeparator) {
                const footnotesContainer = document.querySelector('.footnotes');
                const firstDefinition = document.querySelector('.footnote-definition');
                const insertTarget = footnotesContainer || firstDefinition;
                if (insertTarget) {
                    const separator = document.createElement('hr');
                    separator.className = 'footnotes-separator';
                    insertTarget.parentNode.insertBefore(separator, insertTarget);
                }
            }
        });
    </script>
</head>

<body>
    <h1 class="title">Redundant Attention Heads in Large Language Models For In Context Learning</h1>
    <div class="nav-links">
        <a href="../">
            < back to all posts</a>
    </div>
    <p class="meta">
        Published August 31, 2024
    </p>
    <hr>
    <p>Code findable at this <a href="https://github.com/skunnavakkam/redundant-attention-heads">link</a>. This was also cross posted to lesswrong <a href="https://www.lesswrong.com/posts/Q8KmWzbituyGCkSro/redundant-attention-heads-in-large-language-models-for-in">here</a></p>
<p>In this article, I claim a few things and offer some evidence for these claims. Among these things are:</p>
<ul>
<li>Language models have many redundant attention heads for a given task</li>
<li>In context learning works through addition of features, which are learnt through Bayesian updates</li>
<li>The model likely breaks down the task into various subtasks, and each of these are added as features. I assume that these are taken care of through MLPs (this is also the claim that I’m least confident about)</li>
</ul>
<p>To set some context, the task I’m going to be modelling is the task such that we give a pair of $(x, y)$ in the following format:</p>
<pre style="background-color:#fafafa;color:#383a42;"><code><span>(x, y)\n
</span></code></pre>
<p>where for each example, $y = 2x + 3$. As a concrete example, I use:</p>
<pre style="background-color:#fafafa;color:#383a42;"><code><span>(28, 59)
</span><span>(86, 175)
</span><span>(13, 29)
</span><span>(55, 113)
</span><span>(84, 171)
</span><span>(66, 135)
</span><span>(85, 173)
</span><span>(27, 57)
</span><span>(15, 33)
</span><span>(94, 191)
</span><span>(37, 77)
</span><span>(14, 31)
</span></code></pre>
<p>All experiments here were done with <code>llama-3-8b</code> using TransformerLens running on an A100-40GB unless specified otherwise. </p>
<h1 id="claim-1-language-models-have-many-redundant-attention-heads-for-a-given-task">Claim 1. Language models have many redundant attention heads for a given task</h1>
<p>To probe this, I patch activations from the residual stream of a model given context to a model that doesn’t. As a result, it is possible to see where task formation happens. Initially, without any modifications, the hook point that first works to patch over some semblance of the original task was layer 12. At this layer, it seems like the model learns that $y \approx 2x$.</p>
<p>Ablating all attention heads on layers 10 and 11 of the model given context (which I will now reference as Model A, and let the other be model B) does not change the model’s answer significantly. However, when repeating patching, the first point that works is the portion of the residual stream before layer 14. </p>
<p>This can be confirmed through attention patterns, where backup heads on layer 13 activate very strongly after the initial ablation. Ablating layer 13 does something similar, except this time the first layer that works shifts from 14 to 16, with heads on layer 15 activating very strongly in response.</p>
<p>Ablating layer 15 still results in the model coming to the correct answer. However, patching is different in this case. From patching, on no layer of patching does this behavior where the output should be $\approx 2x$ form. Instead, almost the exact answer is copied over from model A. </p>
<p>Instead, part of the model’s strict response is formed. However, not all of it is formed at this layer, and instead parity is learnt after this. </p>
<p>There are clearly a large number of portions of the model responsible for in-context learning. Otherwise, ablating heads responsible for in-context learning would result in an incorrect answer without redundant heads. This weakly supports Anthropic’s initial hypothesis that in-context learning is driven primarily by induction heads, since we also find induction heads <em>everywhere</em> in large language models. </p>
<h1 id="claim-2-in-context-learning-works-through-addition-of-features-which-are-learnt-through-bayesian-updates">Claim 2. In context learning works through addition of features, which are learnt through Bayesian updates</h1>
<p>This is a weaker claim that the first, and I have less evidence to support it. </p>
<p>I have a good amount of evidence that shows that language models update approximately Bayesianly to in-context learning tasks. More specifically, if you segregate answers into two buckets, $A$ and $B$ where $A$ represents the probability of the correct answer, and $B$ represents the probability of the incorrect answer. $A_i$ represents probabilities after seeing $i$ examples. </p>
<p>With each training example seen, a Bayesian update would be:
$$
A_{i + 1} = \frac{A_i \cdot c} {A_i \cdot c + B_i \cdot (1 - c)}
$$</p>
<p>$$
B_{i + 1} = \frac{B_i \cdot (c - 1)} {A \cdot c + B \cdot (1 - c)}
$$</p>
<p>with the model having some prior for $A_0$, with $B_0 = 1 - A_0$, and $c$ being the model’s expectation that the observation is true.</p>
<p>As a result, </p>
<p>$$
\log(A_{i + 1}) = \log(A_i) + \log(c) - norm
$$</p>
<p>We can now drop the normalization constant, instead saying</p>
<p>$$
A_{n}, B_n = \text{softmax}(\log(A_0) + \sum_{i=0}^n \log(c), \log(B_0) + \sum_{i=0}^n \log(1 - c))
$$</p>
<p>Now, we only need to keep this normalization constant around to ensure that the sizes of $\log(A)$ and $\log(B)$ stay reasonable, since they would lose precision as they decrease. In this way, we can represent Bayesian updates as a sequence of additions, with the softmax built in for free with the language model.</p>
<p>This provides a clean reason for why features seem to be updated Bayesianly. For each example, each previous example confirming the pattern attends to the $i$th example, and adds a constant update term (equivilant to $\log(c) + norm$) to the $i$th example. </p>
<p>Mechanistically, I would expect this to be similar to this feature being added from every pair of examples where the attention head thinks the two examples look similar, thus causing the model’s expectation for $y_j$ to be formed from the addition of $j \cdot feature$</p>
<p>This supports the general linear representation hypothesis. In addition, this also plays nicely with the idea that different features are learnt at different points of the model in-context, since no knowledge of the prior is needed for this to work. This allows for parity, the idea that the output is a number, and the idea that the output is $\approx 2x$ to be added at different points in the model.</p>
<p>One thing I’m still slightly unclear about is how the model is able to crystalize this in the answer, and why patching at different points shows different behavior as to when the model crystalizes the answer (such as during my part in claim 1, where model B started reporting the actual answer, instead of learning the task).</p>
<p>My current hypothesis for this is two part:</p>
<ol>
<li>That the point which is represented by the sum of the features $y \approx 2x$, $ y \bmod 2 \equiv 1$ , that $y$ is a number, and a prior for $x$ maps to $y = 2x + 3$</li>
<li>That before the layer where we see model B reporting the answer to model A’s task, the prior for $x$ formed by model A is already added to the residual stream</li>
</ol>
<h1 id="claim-3-the-model-breaks-down-the-task-into-subtasks-mlps-are-responsible-for-this">Claim 3. The model breaks down the task into subtasks. MLPs are responsible for this.</h1>
<p>Broadly, the algorithm that I have in my head is roughly:</p>
<ol>
<li>
<p>One attention head attends from $x \to y$ for a single example $(x, y)$, and brings information about what $x$ is. Hopefully, and likely, these vectors that convey what $x$ is and what $y$ is are both in the residual stream position of $y$, and approximately orthogonal. I think it’s fairly likely that this is an induction head!</p>
</li>
<li>
<p>Then, an MLP acts on this, breaking down the two directions into a superposition of a bunch of features representing what the transformation $x \to y$ is, with the size of these features approximating how confident the MLP is that the transformation is the correct one.</p>
</li>
<li>
<p>After this, another attention head attends from the $y_i$ token to the token before $y_j$, with $j &gt; i$. This is responsible for adding the feature that corresponds to the transformation, updating it Bayesianly. I think that the size of this update is probably only proportional to the MLPs confidence in the transformation.</p>
</li>
</ol>
<p>My evidence for this is that of the heads that were ablated to show redundancy, the heads that contributed the most in any given layer had attention patterns that looked similar to what you would expect if point 3 were true. </p>

</body>

</html>