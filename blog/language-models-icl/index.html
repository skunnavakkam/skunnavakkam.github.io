<!DOCTYPE html>
<html lang="en">

<style>
    /* images should be limited to  800px, centered, and with padding 20px*/

    img {
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: min(800px, 100% - 40px);
        padding: 20px;
    }

    /* center the body */
    body {
        font-family: 'Roboto', sans-serif;
        margin: 0 auto;
        /* Center the body horizontally */
        padding: 30px;
        box-sizing: border-box;
        line-height: 1.6;
        letter-spacing: 0.5px;
        max-width: max(60%, 600px);
    }

    .content-wrapper {
        /* max width of either 60% or 600px, whichever is larger */
        max-width: max(60%, 600px);
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        font-family: 'Space Mono', monospace;
        margin-bottom: 6px;
    }

    .title {
        font-size: 2.5em;
    }

    h1 {
        font-size: 2em;
    }

    h2 {
        font-size: 1.7em;
    }

    h3 {
        font-size: 1.6em;
    }

    h4 {
        font-size: 1.1em;
    }

    p {
        margin-bottom: 15px;
    }

    em {
        font-style: italic;
    }

    a {
        color: #e1b60e;
        /* Pastel yellow */
        text-decoration: underline;
        font-family: 'Space Mono', monospace;
    }

    u {
        text-decoration: underline;
    }

    strong {
        font-weight: bold;
    }

    .nav-links {
        display: flex;
        margin-bottom: 0px;
        width: 100%;
    }

    .clear {
        font-family: 'Space Mono', monospace;
        background-color: transparent;
        border-width: 0px;
        bottom: 30px;
        left: 30px;
        position: absolute;
        text-decoration: underline;
    }

    /* Brutalist Table Styling */
    table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 20px;
        font-family: 'Space Mono', monospace;
    }

    th,
    td {
        padding: 10px;
        text-align: left;
        border: 2px solid #000;
    }

    pre {
        padding: 10px;
        /* Add padding to all lines of the code block */
        /* Optional: background color for better visibility */

        /* Optional: border for better visibility */
        overflow: auto;
        /* Optional: add scrollbars if content overflows */
        /* Add black shadow with no blur */

    }

    ul {
        list-style: none;
        padding-left: 30px;
    }

    ul li {
        position: relative;
        padding-left: 30px;
        margin-bottom: 10px;
    }

    ul li::before {
        content: '■';
        position: absolute;
        left: 0;
        color: #000;
        font-size: 1.2em;
        font-weight: bold;
    }



    /* Nested Lists */
    ul ul,
    ol ol,
    ul ol,
    ol ul {
        margin-top: 10px;
    }

    th {
        background-color: #000;
        color: #fff;
        font-weight: bold;
        text-transform: uppercase;
    }

    tr:nth-child(even) {
        background-color: #f0f0f0;
    }

    /* Responsive table */
    @media screen and (max-width: 600px) {

        table,
        thead,
        tbody,
        th,
        td,
        tr {
            display: block;
        }

        thead tr {
            position: absolute;
            top: -9999px;
            left: -9999px;
        }

        tr {
            margin-bottom: 10px;
            border: 2px solid #000;
        }

        td {
            border: none;
            border-bottom: 1px solid #000;
            position: relative;
            padding-left: 50%;
        }

        td:before {
            content: attr(data-label);
            position: absolute;
            left: 6px;
            width: 45%;
            padding-right: 10px;
            white-space: nowrap;
            font-weight: bold;
            text-transform: uppercase;
        }
    }
</style>

<head>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Space+Mono&display=swap"
        rel="stylesheet">
    <meta charset="UTF-8">
    <script data-goatcounter="https://skunnavakkam.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- <link rel="stylesheet" href="https://sudarsh.com/common.css"> -->
    <title>Language Models Update Based on In-Context Learning</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
        integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Language Models Update Based on In-Context Learning" />
    <meta name="twitter:description" content="A look at how language models update their priors based on in-context examples" />
    <meta name="twitter:url" content="https://sudarsh.com" />
    

    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
</head>

<body>
    <h1 class="title">Language Models Update Based on In-Context Learning</h1>
    <div class="nav-links">
        <a href="../">
            < back to all posts</a>
    </div>
    <p class="meta">
        Published August 21, 2024
    </p>
    <hr>
    <p>Since I’m going to be talking about two different types of models, one which is a language model and the other which is the language model’s model of the context, I’m going to refer to language models as LMs in this post. I’m also still in the process of writing this. If you don’t see source code in the next few months, or you want immediate access to source code, just shoot me an email :)</p>
<p>LMs are next token predictors. Through giving the language model a series of tasks in context, where the output of the task is a single token, we can understand the LM’s predictions over all possible tokens, and understand how likely the LM thinks that a given token is the correct answer for the task.</p>
<p>Through this, we can also measure how much information we need to feed a LM to allow it to learn a task. If the correct answer for task $i$ is $x_i$, we give the LM 
$$
-\log(P_{i - 1}(x_i))
$$
where $p_i(x)$ is the probability the LM assigns to the token $x$ after the $i$th example. This is just the surprisal. Then, we can find the total amount of information given to the LM to learn the task to be
$$
\sum_{i = 1}^{\infty} -\log(P_{i - 1}(x_i))
$$
Practically, the surprisal goes to zero very quickly if the LM is able to grasp the task, or the LM is never able to grasp the task, and no amount of information is sufficient.</p>
<p>The following is a graph of LM surprisal to learn the task $f(x) = 2x + 3$, and we can see that the surprisal quickly drops to near zero, and very little extra information is given to the LM.</p>
<p><img src="https://sudarsh.com/blog/language-models-icl/lm-surprisal.png" alt="" /></p>
<p>It is also interesting to understand how the language model updates it’s priors. We only need to consider two cases: when the correct answer is $2x + 3$ and when the correct answer is not. We can call these two states $A$ and $B$ respectively. Initially, the model has some prior $\theta$ over the two, such that $P(A | \theta) + P(B | \theta)$ = 1. To give more granularity, we can say that $\theta_i$ is the prior after $i$ training examples.</p>
<p>Now, we can model this using Bayesian statistics.</p>
<table><thead><tr><th></th><th>A</th><th>B</th></tr></thead><tbody>
<tr><td>Prior</td><td>$\theta_A$</td><td>$\theta_B$</td></tr>
<tr><td>Observation</td><td>$c$</td><td>$1-c$</td></tr>
<tr><td>Posterior</td><td>$\theta_A \times c$</td><td>$\theta_B \times (1- c)$</td></tr>
</tbody></table>
<p>Our final probability for $\theta_{i +1, A}$ is
$$
\frac{\theta_{i, A} \times c}{\theta_{i, A} \times c + \theta_{i, B} \times (1- c)}
$$
Using this bayesian model to “simulate” the LM updating it’s priors, we get a similar looking graph of surprisal over time!</p>
<p><img src="https://sudarsh.com/blog/language-models-icl/simulated-surprisal.png" alt="alt text" /></p>
<p>We can also look at the LM’s probability of A:
<img src="https://sudarsh.com/blog/language-models-icl/lm-prior.png" alt="" /></p>
<p>and compare this to our simulated, “bayesian” model:
<img src="https://sudarsh.com/blog/language-models-icl/simulated-prior.png" alt="" /></p>
<p>Since language model outputs end up being softmaxed, we can work with logprobs.
$$
\log(A) := \log(A) + \log(c) + norm
$$</p>
<p>$$
\log(B) := \log(B) + \log(1 - c) + norm
$$</p>
<p>$norm$ is some normalization constant that allows values to stay resonable. $A + B$ don’t need to equal 1 since they get normalized at softmax.</p>
<p>Since the unembedding matrix is just a linear transformation on the logits, I hypothesize that the sizes of the features corresponding to $A$ and $B$ increase by this roughly constant amount during each example. </p>
<p>Logit lens and feature patching allow some more interesting insights. From logit lens, the model only crystalizes that the output token is an integer after the 20th layer. From patching, the model learns that $y \approx 2x$ around the 10th / 11th layer. I initially hypothesized that this was the result of the first forming some internal model of the task from the 11th to 16th layer, then converting this to the actual answer after the 20th layer. I don’t think that this is true anymore.</p>
<p>I think that it’s inappropriate to think of the idea that $y$ is always an integer and not some other token as a form of the model crystallizing it’s answer. It seems more natural to think that this is just another feature that the model learns through ICL. To really test this, we can ablate different heads in the model. Specifically, L13H6, L13H27, L10H5 and L10H7.</p>
<p>The model has induction heads that attend to previous tokens. This likely allows for the model to say “here is where I need to learn from”. </p>

</body>

</html>