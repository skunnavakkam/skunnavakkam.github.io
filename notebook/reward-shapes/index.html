<!DOCTYPE html>
<html lang="en">

<style>
    /* images should be limited to  800px, centered, and with padding 20px*/

    img {
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: min(800px, 100% - 40px);
        padding: 20px;
    }

    /* center the body */
    body {
        font-family: 'Roboto', sans-serif;
        margin: 0 auto;
        /* Center the body horizontally */
        padding: 30px;
        box-sizing: border-box;
        line-height: 1.6;
        letter-spacing: 0.5px;
        max-width: max(60%, 600px);
    }

    .content-wrapper {
        /* max width of either 60% or 600px, whichever is larger */
        max-width: max(60%, 600px);
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        font-family: 'Space Mono', monospace;
        margin-bottom: 6px;
    }

    .title {
        font-size: 2.5em;
    }

    h1 {
        font-size: 2em;
    }

    h2 {
        font-size: 1.7em;
    }

    h3 {
        font-size: 1.6em;
    }

    h4 {
        font-size: 1.1em;
    }

    p {
        margin-bottom: 15px;
    }

    em {
        font-style: italic;
    }

    a {
        color: #e1b60e;
        /* Pastel yellow */
        text-decoration: underline;
        font-family: 'Space Mono', monospace;
    }

    u {
        text-decoration: underline;
    }

    strong {
        font-weight: bold;
    }

    .nav-links {
        display: flex;
        margin-bottom: 0px;
        width: 100%;
    }

    .clear {
        font-family: 'Space Mono', monospace;
        background-color: transparent;
        border-width: 0px;
        bottom: 30px;
        left: 30px;
        position: absolute;
        text-decoration: underline;
    }

    /* Brutalist Table Styling */
    table {
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 20px;
        font-family: 'Space Mono', monospace;
    }

    th,
    td {
        padding: 10px;
        text-align: left;
        border: 2px solid #000;
    }

    pre {
        padding: 10px;
        /* Add padding to all lines of the code block */
        /* Optional: background color for better visibility */

        /* Optional: border for better visibility */
        overflow: auto;
        /* Optional: add scrollbars if content overflows */
        /* Add black shadow with no blur */

    }

    ul {
        list-style: none;
        padding-left: 30px;
    }

    ul li {
        position: relative;
        padding-left: 30px;
        margin-bottom: 10px;
    }

    ul li::before {
        content: '■';
        position: absolute;
        left: 0;
        color: #000;
        font-size: 1.2em;
        font-weight: bold;
    }



    /* Nested Lists */
    ul ul,
    ol ol,
    ul ol,
    ol ul {
        margin-top: 10px;
    }

    th {
        background-color: #000;
        color: #fff;
        font-weight: bold;
        text-transform: uppercase;
    }

    tr:nth-child(even) {
        background-color: #f0f0f0;
    }

    /* Responsive table */
    @media screen and (max-width: 600px) {

        table,
        thead,
        tbody,
        th,
        td,
        tr {
            display: block;
        }

        thead tr {
            position: absolute;
            top: -9999px;
            left: -9999px;
        }

        tr {
            margin-bottom: 10px;
            border: 2px solid #000;
        }

        td {
            border: none;
            border-bottom: 1px solid #000;
            position: relative;
            padding-left: 50%;
        }

        td:before {
            content: attr(data-label);
            position: absolute;
            left: 6px;
            width: 45%;
            padding-right: 10px;
            white-space: nowrap;
            font-weight: bold;
            text-transform: uppercase;
        }
    }
</style>

<head>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Space+Mono&display=swap"
        rel="stylesheet">
    <meta charset="UTF-8">
    <script data-goatcounter="https://skunnavakkam.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- <link rel="stylesheet" href="https://sudarsh.com/common.css"> -->
    <title>Why does my reward look like *that*?</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
        integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Why does my reward look like *that*?" />
    <meta name="twitter:description" content="I’ve been training language models with reinforcement learning recently, and see some
interesting behavior with my rewards. 
Let’s set up the problem by saying that I’m running Group Relative Policy O…" />
    <meta name="twitter:url" content="https://sudarsh.com" />
    

    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
</head>

<body>
    <h1 class="title">Why does my reward look like *that*?</h1>
    <div class="nav-links">
        <a href="../">
            < back to all posts</a>
    </div>
    <p class="meta">
        Published July 17, 2025
    </p>
    <hr>
    <p>I’ve been training language models with reinforcement learning recently, and see some
interesting behavior with my rewards. </p>
<p>Let’s set up the problem by saying that I’m running Group Relative Policy Optimization (GRPO)
with two reward functions $\phi$ and $\varphi$, with my final reward $r = 4 \cdot \phi + \varphi$.
I’m using weights on my rewards because I wanted to encourage $\phi$-like behavior in my model.</p>
<p>This is what my rewards look like.</p>
<p><img src="https://sudarsh.com/notebook/reward-shapes/rewards.png" alt="Reward Shapes" /></p>
<p>The shape here isn’t very important. What you should notice is that $\phi$ climbs while $\varphi$
goes down. Net reward also goes up, although this isn’t pictured.</p>
<p>This makes sense if $\phi$ and $\varphi$ rewarded opposite things, but I don’t think this is the case.
I think $\phi$ and $\varphi$ are orthogonal, and that you can have many different values of $\varphi$ 
for a given value of $\phi$.</p>
<p>Instead, I have another explanation here. </p>
<p>Let $u, v$ be vector directions in activation / weight space which roughly represent $\nabla \phi$ and $\nabla \varphi$ respectively (a direction that maximizes $\phi, \varphi$ respectively). Let the model start off with some
parameter set $\theta_0$ and represent the model as a function $M$ of $\theta$, with two components $M_u(\theta) = \phi(M(\theta))$ and $M_v(\theta) = \varphi(M(\theta))$. </p>
<p>I make the load-bearing claim that the KL divergence required to increase $u$ is larger than the KL divergence required to decrease $u$ and so forth for $v$. Namely, for $\theta_{+u}, M_u(\theta_{+u}) &gt; M_u(\theta_0)$ and $\theta_{-u}, M_u(\theta_{-u}) &lt; M_u(\theta_0)$, </p>
<p>$$
KL(\theta_{+u} || \theta_0) &gt; KL(\theta_{-u} || \theta_0)
$$</p>
<p>This is a strong claim, but I think holds  true. For example, small distributional changes can cause the model to forget a sign when doing math, but making non-trivial gains in math is hard. This leads to the model being less likely to generate things with $+u, +v$ and more likely to generate things with $-u, -v$. Some version of this can be strengthened using Markov’s variance bound.</p>
<p>Then, look at the reward in the cases $+u, -u, +v, -v$. (table 1)</p>
<table><thead><tr><th></th><th>+ $\Delta v$</th><th>- $\Delta v$</th></tr></thead><tbody>
<tr><td>+ $\Delta u$</td><td>+</td><td>+</td></tr>
<tr><td>- $\Delta u$</td><td>-</td><td>-</td></tr>
</tbody></table>
<p>In both cases, where $\Delta u$ is positive, the reward, and thus advantage is positive. However, since $p(\delta u &gt; 0, \delta v leq 0) &gt; p(\delta u &gt; 0, \delta v &gt; 0)$, the model is more likely to optimize on the $+u, -v$ case.</p>
<p>However, this will not always be true. If $\varphi$ ever gets too low, it becomes very easy to optimize for $\varphi$ and the expectations work out in favor of $\varphi$. Thus, even though the $\varphi$ reward is decreasing, accounting for it in the reward function causes pressure to maintain / not fall too much. </p>

</body>

</html>