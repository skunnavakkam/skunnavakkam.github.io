{
  "settings": {
    "passageFolder": "",
    "defaultPassageSeparator": "\\n\\n---\\n\\n",
    "defaultPassageFrontmatter": "%r:\\n",
    "logApiCalls": false,
    "modelPresets": [
      {
        "name": "Together",
        "provider": "ocp",
        "model": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "contextLength": 8192,
        "apiKey": "f78be56b2a561b90cfff2ec8912e8a7c84cf763415e3c10c242554cbe3ed72eb",
        "url": "https://api.together.xyz/v1"
      }
    ],
    "modelPreset": 0,
    "visibility": {
      "visibility": true,
      "modelPreset": true,
      "maxTokens": true,
      "n": true,
      "bestOf": false,
      "temperature": true,
      "topP": false,
      "frequencyPenalty": false,
      "presencePenalty": false,
      "prepend": false,
      "systemPrompt": false,
      "userMessage": false
    },
    "maxTokens": 60,
    "temperature": 1,
    "topP": 1,
    "frequencyPenalty": 0,
    "presencePenalty": 0,
    "prepend": "<|endoftext|>",
    "bestOf": 0,
    "n": 5,
    "systemPrompt": "The assistant is in CLI simulation mode, and responds to the user's CLI commands only with the output of the command.",
    "userMessage": "<cmd>cat untitled.txt</cmd>",
    "showSettings": false,
    "showSearchBar": false,
    "showNodeBorders": false,
    "showExport": false
  },
  "state": {
    "religions_seem_to_be_convergent.md": {
      "current": "7c79fa8e-4070-46bf-a639-1f5447e61732",
      "hoisted": [],
      "searchTerm": "",
      "nodes": {
        "b2d40e35-b26e-4e05-9915-3b60c8436395": {
          "text": "+++\ntitle=\"Religions seem to be convergent\"\ndate=2024-08-03\n+++\n\nThere is this phenomenon where people do bad things, and this phenomenon happens fairly often. To be more concrete, there are often times where people take actions that misalign with my value system. I think that this ",
          "parentId": null,
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723297928918
        },
        "fcd3e4b5-088a-4ed4-9d01-2417fe6f3fd2": {
          "text": "\n",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null
        },
        "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff": {
          "text": "mode of action is very defendable. Furthermore, these actions can be very extreme, and may still not color my impression of their character or of them b",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723324217552
        },
        "204f2eb4-b998-4f9a-afaf-ce7cdc1dd94b": {
          "text": "If I were to ask you, I think you'd deem this practice unholy and awful, but it was in fact fully embraced by Indian society. Abominable! But now, as we look back, we can realize that this practice is not horrid, nor value-dethroning.",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "15ef7782-51a6-4792-8a56-ffd1165e759e": {
          "text": "Basically, a woman would commit suicide in a very specific ritual. You have to think, what kind of woman would do this? I would claim that the (primary) answer to this is a bad person. But, it is perhaps necessary to clarify that this woman was very far from bad in her",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "e8a923e2-605d-4c35-bd92-3dbd5a11d7b4": {
          "text": "Specifically, a newly widowed woman would climb atop her husband's funeral pyre and join his commitment into another kingdom. This tradition was outlawed by the British Empire in the 1800s, but it made a large enough impression on the culture that it is still a thing we know of today",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "cac3695d-8e1e-4740-b9b7-d02cfcd4bbad": {
          "text": "It's most known when a woman would, while grieving, go and throw herself into her husband's funeral pyre. At first glance, it can be hard to put myself into these people's position. I can't imagine throwing myself into a fire. Resorting to death instead of confronting problems?",
          "parentId": "b2d40e35-b26e-4e05-9915-3b60c8436395",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "7c79fa8e-4070-46bf-a639-1f5447e61732": {
          "text": "eing virtuous.\n\nThe crusades were a violent, bloody, and murderous endeavor. \n",
          "parentId": "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723324219788
        },
        "6e0d1c01-156f-4eab-b022-fad3c1e50546": {
          "text": "To my sensibilities, this practice sounds extremely horrific and a net negative. However, after lots of time passing over this ancient practice, it has a more cheery feel to it. Like of course women were committing self sacrifice to show their virtue, it seems a lot more common. To contextualize",
          "parentId": "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "8c2dfc19-9b01-42ca-b7c7-1ce0d9561dcf": {
          "text": "\n\nSo what is this phenomenon? \n\nIn Satnam Kaur's book \\[Sacredness of Trees]\\[0], she states that _\"In the knowledge of the one, the unity of the all is apparent\"_. In the book, she says, \"... again, it is not feminine because",
          "parentId": "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "a38da535-9345-42f0-bec5-37d6385040ed": {
          "text": "\n\nAt first glance, this practice seems very heinous, it is imposed upon women, and also doesn't help with anything. But if you put yourself into the shoes of that culture, it is almost perfectly virtuous. Women in the Indian kingdoms were expected to live in a very particular way,",
          "parentId": "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "c6187ca9-045d-41fa-84aa-6f27cf2f5e93": {
          "text": "Sometimes this would be after the war when their husbands died defending their cities and sometimes it would be during the war. If wholly accepted, it seems like this implies that when Sati Warriors threatened Military defeat to the inhabitants of a city, then this would cause a societal pressure that would cause women (of",
          "parentId": "b8e7f5a9-7b8a-4612-bb8b-4a2ace88d0ff",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        }
      },
      "generating": null
    },
    "reval.md": {
      "current": "69f3e216-681b-4f77-86a4-74c2864cefc9",
      "hoisted": [],
      "searchTerm": "",
      "nodes": {
        "69f3e216-681b-4f77-86a4-74c2864cefc9": {
          "text": "+++\ntitle=\"reval\"\ndate=2024-07-21\n+++\n\nReval is a single-file evaluation framework for evaluating large language models on your own data.\n\n# motivation\n\nMost benchmarks don't translate well into real world performance, on your tasks. There are multiple benchmarks where claude-3.5-sonnet scores higher than gpt-4o, and vice versa. How should you know which one of the two to use for your business?\n\nRight now, you don't, and Reval solves that. \n\n# usage\n\nThe most simple use of Reval is\n\n```bash\npython reval.py --task=\"path/to/data.csv\" --models=[\"model_name\"]\n```\n\nYou may have to set your `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, and `TOGETHER_API_KEY` env variables.\n\nBy default, Reval supports all OpenAI, Anthropic, and together.ai models. It also support common disambiguations (eg. `claude-3.5-sonnet` calls sonnet, instead of needing to use the internal name)\n\nReval can be called in a few simple modes: Single and Arena. Single mode evaluates a single model against your data, while Arena mode evaluates 2 or more models on your data. Reval defaults to Single mode, and Arena mode can be enabled with the `--mode=\"arena\"` flag. In this case, `--models` should be a list >2 models.\n\n### data schema:\n\nYour `data.csv` can take on a few of the following forms (please note that column names must be exact). However, column orders may be arbitrary. Columns that do not fit in the scheme will be ignored:\n\n#### schema 1\n```\n| tasks |\n| ----- |\n```\n\nIn this case, Reval generates failure and success criteria for each task in tasks. This is then converted to a table like schema 3, and saved in the current directory\n\n#### schema 2\n\nFor more control over your data, you can either provide good and bad examples, or your own success and failure criteria (see schema 3)\n\n```\n| tasks | good-examples |\n| ----- | ------------- |\n```\n\n```\n| tasks | bad-examples |\n| ----- | ------------ |\n```\n\n```\n| tasks | good-examples | bad-examples |\n| ----- | ------------- | ------------ |\n```\n\nare all valid schema here, and Reval will generate success and failure criteria based on it. \n\n#### schema 3\n\n```\n| tasks | success-criteria | failure-criteria |\n| ----- | ---------------- | ---------------- |\n```\n\nIf either success critieria or failure criteria don't exist, Reval will generate a column for it. This is the jumping off point from where models are evaluated. \n\n### output schema\n\nReval will save one `.csv` in the current directory of your `data.csv` with success and failure criteria included. If success and failure criteria already exist in `data.csv`, this will be a duplicate. \n\nIn addition, a file `results.json` will be generated. The schema of this file will differ by mode. \n\nIn Single mode:\n```json\n[\n    {\n        \"model\": model_name<str>, \n        \"score\": score<int>\n    }\n]\n```\n\nIn Arena mode:\n```json\n[\n    {\n        \"model\": model_name<str>, \n        \"score\": score<int>, \n        \"elo\": elo<int>, \n        \"num_matches\": num_matches<int>\n    }\n]\n```\n\nA raw data file `raw_data.json` will also be created. Output schema for that will be included later.\n\n### Parameters\n\n| Parameter | Description | Type |\n| --------- | ----------- | ---- |",
          "parentId": null,
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null
        }
      },
      "generating": null
    },
    "brain-cognition.md": {
      "current": "ef372d66-73aa-411b-bf8e-28284564e412",
      "hoisted": [],
      "searchTerm": "",
      "nodes": {
        "8b9d681b-a872-4e1e-a82b-10bb12f73c6c": {
          "text": "+++\ntitle=\"How neuromorphic **ARE** current neural networks?\"\ndate=2024-08-10\n+++\n\n*part of this blog post was written with the help of Llama-3.1-405b*\n\nNeural networks started off as an attempt to mimic the structure of the brain. The initial perceptron was designed to replicate one neuron in the brain. For a significant amount of time, perceptrons and neural networks were not considered to be the state of the art with regards to artificial intelligence. The current paradigm of deep learning became the paradigm due to Yan LeCun's paper on backpropogation and Hinton's work on AlexNet. \n\nThese deep networks have scaled to be extremely large, measuring trillions of parameters, and showing \"hints of AGI\". ",
          "parentId": null,
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723326578739
        },
        "5243cefc-4fcf-48f1-b732-8df928d6c129": {
          "text": "\n\nNeural networks started off as an attempt to replicate the brain, allowing for the combination ",
          "parentId": "d3b10a59-2fd4-4066-8b4a-72dd59179726",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null
        },
        "d3b10a59-2fd4-4066-8b4a-72dd59179726": {
          "text": "\n\n",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null
        },
        "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8": {
          "text": "How close are our current architectures to mapping to the brain?\n\n### Scale\nThe brain is larger than any SOTA models. The brain is estimated to have around $1 \\times 10^{11}$ neurons, with no good bound on the number of synapses. ",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723327169891
        },
        "0a207960-26d6-445e-a284-8ae1f2cc34f1": {
          "text": "But it's hard not to see the falacy in finally attempting to understand or even reason about how these large systems work. When they are falable, we appologize and explain that this is the nature of deep learning and large models. \n\nHowever, since the basis of these large models are",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "c625fc19-858d-4e5d-8537-24dcbf782ade": {
          "text": "Do scaling up models infinitely lead to a super-intelligent being? Will current models start exhibiting more biologically realistic intelligence in their current form?\n\nWe can evaluate the \"neuromorphicness\" of current models based on the 8 reasonable requirements outlined in this essay by Jeff Hawkins - ex director of",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "d6eec3dc-3876-45e8-99ea-e62ead0a5fdd": {
          "text": "Some researchers argue that deep learning approaches are fundamentally flawed, and that non-determined approaches and fully neuromorphic networks are the approaches that will realize AGI. \n\nIt's common to hear that deep neural networks are \"not very brain-like\", but let's explore why. \n\n### 1.",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "d84cd0a4-72da-43ea-bbe1-b5aa199e28f8": {
          "text": "Specifically, tokens such as in the case of Whisper, PaLM and Llama, show that scaling sfty large enables the neural network to perform tasks without having to be explicitly programmed for them.\n\nHence, today, deep learning has become the top paradigm in AI research, simply explaining \"intelligence\"",
          "parentId": "8b9d681b-a872-4e1e-a82b-10bb12f73c6c",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "7a1e8c81-89e3-4b32-93d1-d6991054b05b": {
          "text": "For sake of argument, let's say the brain has about $10^{11} \\cdot 10^{4} = 10^{15}$ synapses. This is likely slightly too high, with most estimates ranging between $100$ trillion and $700$ trillion synapses. If each synapse in the brain were treated as a single parameter, this puts the brain at the scale of the largest current model, which was the BaGuaLu model, at ~170 trillion parameters.  However, neurons in the brain are significantly more complex than ANN neurons. \n\nA recent paper claimed that it took between 5 - 8 layers and 1000 DNN neurons in a DNN to approximate a neuron. I'm slightly doubtful that this claim holds - much of this complexity could be overhead, and unnecessary. For example, it may take multiple ReLU units to approximate one GeLU unit, but we would not consider a network using GeLU to have multiple times the number of parameters as an equivalently sized ReLU network. However, I don't think that the main message, that neurons in a neural network are more complex than ANN neurons, is wrong. ",
          "parentId": "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328606356
        },
        "a7fd4483-0136-4a12-96a4-69b03cef6c47": {
          "text": "Papers have claimed that neural networks will have to be several orders of magnitude larger to match or mirror the size of the neocortex. One widely cited paper claims that models need to consist of 14.5Tn parameters to match the size of the neural networks in the brain.\n\nHere is the",
          "parentId": "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "d9b30cff-0abc-4295-9ffa-9124a1358a98": {
          "text": "This is in contrast to the largest models which are $10^5$.\n\nThe vast majority of this area is actually wired up into the visual system, which in humans, is nearly half of the entire brain. So the largest models are within a few OOM of matching the number of neurons in",
          "parentId": "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "1a78b073-953f-4a1f-bd6b-581b5c1145e6": {
          "text": "The largest models of today are difficult to report, especially with open source due to the share scale but the difference between SOTA and the size of the human brain is hard to downplay. It is fair to say the model isn't too large and it's clear we don't need the size of",
          "parentId": "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "3afb9972-a050-458d-8f9f-6c535016e370": {
          "text": "Even the largest model cannot compare in terms of sheer size. As some brains are much larger than other brains, there is no \"threshold\" for intelligence where the neural network has finally been large enough.\n\n### Sparsity\nAfter the initial neuron was designed as a multilayer perceptron, the T",
          "parentId": "d6c56784-382b-4d83-ad3a-3dd11fc8e0c8",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "166429c5-51e4-44d1-bda3-c26cf73cc285": {
          "text": "",
          "parentId": "7a1e8c81-89e3-4b32-93d1-d6991054b05b",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328630898
        },
        "1e179a3c-b672-4215-969d-e292fa362baf": {
          "text": "This brings us to the second point: the brain is way more efficient.\n\n### Efficiency\nBoth power efficiency and data efficiency are areas where the brain far exceeds the power of a current-era neural network. The brain is able to learn complex concepts quickly, in very few data points, in much shorter amounts",
          "parentId": "7a1e8c81-89e3-4b32-93d1-d6991054b05b",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "575cfab2-09bb-47ab-aa7f-948cc6b35671": {
          "text": "Variability in the amount of calcium channels, differences in myelination and axon transmission speed, bushy vs non-bushy neurons, all sorts of complexities that most deep neural networks do not model. However, putting a single parameter to each of these is likely a much overestimation -",
          "parentId": "7a1e8c81-89e3-4b32-93d1-d6991054b05b",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "07565beb-7a65-4894-a4f9-79bf86b1b20a": {
          "text": "Even though though not every neuron in a neural network may be as complex as a neuron in the brain, we are likely scaling the size of components correctly.\n\n### Synaptic Plasticity\nSynaptic plasticity generally refers to the idea that individual weights in a neural network should change based on the idea that",
          "parentId": "7a1e8c81-89e3-4b32-93d1-d6991054b05b",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "2a91a413-3ddc-4104-92ea-d287119fa36d": {
          "text": "Neural networks use linear transformations with learned parameters as a \"single layer\", whereas neural networks approximate arbitrary loop functions with parameters. This requires a significant more number of operations.\n\nThe brain is then significantly larger than our current models, even if we consider the disturbingly large BaGuaLu model. If we",
          "parentId": "7a1e8c81-89e3-4b32-93d1-d6991054b05b",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "6815acaa-234d-4f02-ace4-3b47d9ec9952": {
          "text": "",
          "parentId": "166429c5-51e4-44d1-bda3-c26cf73cc285",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328642854
        },
        "6344cbf9-1c2b-422f-9899-e33275bc2790": {
          "text": " spikes, which are governed by very different principles than the ReLUs, GeLUs, and sigmoids we're familiar with. ANNs ignore this intricacy entirely, faking full firing and recovery with positive forward passes.\n\nHowever, more recent techniques, like self-modulated sparse networks are",
          "parentId": "166429c5-51e4-44d1-bda3-c26cf73cc285",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "e5b8660a-6b33-4fdc-85e5-a67b6fc1e703": {
          "text": " a series of impulses. Most current networks pass inputs and outputs backward and forward simultaneously. In parallel. The brain is untrue space as ANNs are - computation on traditional computer architectures is accomplished by utilizing multiple CPUs that work on pass through layers in parallel. Biological computing is organized in serial, everything happens after",
          "parentId": "166429c5-51e4-44d1-bda3-c26cf73cc285",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "b5807ab4-579f-48b1-af03-aecd50e80c22": {
          "text": " beats or cycles, which make time domain signal processing very useful.  In the nervous system, spikes are rare events and the spiking frequency is often low.  As an example, if a neuron is emitting a high-speed spike, this could peak round $200ms$. If this neuron is only",
          "parentId": "166429c5-51e4-44d1-bda3-c26cf73cc285",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "8a9623fb-f220-435f-805b-d02b3a9a9c6a": {
          "text": " an ELF and LFE structure, and time domain information preserves much more information than aggregated information based on firing count or density.  For one sub-measurement of cognitive function, research has shown that quantifying certain types of temporal dynamics can account for much more of the explains additional unique variance of neural activity",
          "parentId": "166429c5-51e4-44d1-bda3-c26cf73cc285",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "f9b9e1a8-37b2-45b1-9f55-bc92a81fe921": {
          "text": " the Hebbian Heuristic: \"neurons that fire together, wire together\". While there exist forms of neural networks that are structured in the time domain, like recurrent neural networks, RNNs are much more rarely used. These time domain structures are more biologically possible in neuromorphic chips",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null
        },
        "aa87d572-8b27-45b6-8238-aeff8f6b19e7": {
          "text": "Therefore, the neuron-synapse equivalency is not a perfect 1-1 equivalency either. \n\nTaking these into account, neural networks seem to be under several orders of magnitude away from the brain in terms of scale.\n\n### Functional similarity\nDespite both having \"neurons\" and backprop",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328650426
        },
        "7d78b627-65d9-405b-85bd-267dd5f7d356": {
          "text": "\n\nAs such, I think that neural networks would have to become orders of magnitude larger before they reach the same size and weight of the brain.\n\n### Connections' strength and structure\nIn a very ~empirical~ sense, the DNNs are highly connected. While DNNs have learned to",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328655260
        },
        "89d846b6-9981-4316-85f9-81afb8e385a7": {
          "text": "I think a safe estimate for a conversion is $1$ neuron $= 4 - 8$ DNN neurons. This put the total number of parameters in the brain around one quadrillion parameters. However there is good research that suggests the brain is able to approximate functions that should take a much",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328655959
        },
        "66898c32-4982-4470-8a78-5f65e0e8bac3": {
          "text": "\n\nHowever, with this all being said, scaling is still the least credible point of difficulty for our current networks to overcome in approximating the brain. If it takes 5 - 8 layers to approximate a neuron in the brain, and our current best model is ~170T parameters, we could",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328659110
        },
        "29ad4af7-d139-4e51-a290-98ace0e02b6e": {
          "text": "\n\nFirstly, my ReLU-GeLU example was a strawman. When removing NMDA channels from the \"base model of reality\" in the paper, the complexity in the neuron decreased sharply, and only one layer was needed to model the neuron. Secondly, neurons do not act like sole activation functions, and instead have significant signal processing ability. ",
          "parentId": "6815acaa-234d-4f02-ace4-3b47d9ec9952",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328853142
        },
        "5f587655-dffc-4cf6-88fe-93c25f15fc8d": {
          "text": "The brain's ability to respond to certain frequencies of sounds could be solely a result of the signal processing abilities of the neurons themselves, as shown in one paper which studied harmonic response in neurons. \n\nLastly, while backpropagation has been shown to occur in the brain, its not exactly the same math",
          "parentId": "29ad4af7-d139-4e51-a290-98ace0e02b6e",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328859414
        },
        "321d8e99-1b92-4952-8074-e48a294b00eb": {
          "text": "As an example, neocortical neurons will suppress the noise given a spike signal, which is similar to ResNets ability to turn noise into \"subsignal\". Neocortical neurons also may also be able to serve as a LOW-pass and bandpass filter for the data, similar to",
          "parentId": "29ad4af7-d139-4e51-a290-98ace0e02b6e",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328871741
        },
        "033debc2-01af-48e5-adb0-63bd518cced4": {
          "text": "\n\n\n\nWith that covered, it is difficult to determine exactly how complex each neuron is to model, as this varies greatly depending on the specific behavior researched. For a standard complexity neuron. conservatively, ~20 - 100 artificial neurons would be needed to model a single true neuron. Scaling this up to",
          "parentId": "29ad4af7-d139-4e51-a290-98ace0e02b6e",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328870575
        },
        "a708c366-b7b5-4560-a656-72904894993e": {
          "text": "Using optogenetics to excite one individual neuron at differing frequencies causes differing effects. In other words, one neuron is able to differentiate different spike trains, leading to different outcomes. Lastly, chemical signals, which require multiple types of cells to act in coordination, would almost certainly require much more complex models",
          "parentId": "29ad4af7-d139-4e51-a290-98ace0e02b6e",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328872541
        },
        "d16f0af9-4657-476a-b554-8ecef84fbee9": {
          "text": "Neurons function differently given different time-domain signals, and have the ability to do XOR operations on these signals, which is not possible given a single ReLU unit. Lastly, and very importantly, neurons are stateful. Different neurons serve distinct purposes. As an example, ",
          "parentId": "29ad4af7-d139-4e51-a290-98ace0e02b6e",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328996490
        },
        "3f0d5bd0-09c6-42da-991e-347fe6d38ec2": {
          "text": "a subset of neurons known as 'place cells' are attuned to specific locations in the brain and only fire when the animal recieves a simalar sensory input to said location. This \"excitability\" is a quality that current ANN neurons lack.\n\nThus, to approximate the brain would take an",
          "parentId": "d16f0af9-4657-476a-b554-8ecef84fbee9",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723328999897
        },
        "0a928dba-4ab9-46c5-b03e-1fbdb1c4c481": {
          "text": "sensory neurons primarily act as detectors, outputting binary signals if a signal passes their threshold, and repetitive neurons output periodic signals. \n\nThese results indicate that as an upper bound, which is likely highly incorrect, we need at the minimum 5x the depth + parameters of our current largest model, which",
          "parentId": "d16f0af9-4657-476a-b554-8ecef84fbee9",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "bdc5bbdc-b8e7-472e-9c17-eaa1d3e74bff": {
          "text": "neurons in the basal ganglia have ring attractors, while some neurons in the hypothalamus can activate only once. Most of the newer activation functions (GeLU, Mish, Soft-Linear, Catena) do not have state as part of the functional.\n\nEnsemble learning is also required to",
          "parentId": "d16f0af9-4657-476a-b554-8ecef84fbee9",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        },
        "ef372d66-73aa-411b-bf8e-28284564e412": {
          "text": "different neurons use different neurotransmitters. Thus, synapses are a bad analogy to parameters when considering model complexity. \n\nI don't have a strong prior for the complexity of the brain, but I would estimate that it would be equivalent to a model with around $10^{16}$ or $10^{17}$ neurons. \n\n### Control of Cognition\nControl of cognition is not something unknown to transformers. Mixture of Experts, Mixture of Depths, etc does just this. However, the extent to which the \n\n",
          "parentId": "d16f0af9-4657-476a-b554-8ecef84fbee9",
          "collapsed": false,
          "unread": false,
          "bookmarked": false,
          "searchResultState": null,
          "lastVisited": 1723347128256
        },
        "90568e4b-b095-444f-9c13-aea70c3b6d7f": {
          "text": "Purkinje neurons cannot be found in the same roles in the brain. Their structure each make them better suited to certain tasks which are present in the cerebellum, so even although they can be \"modeled\" by different sized transformers, much **information is lost**.  \n\nGiven this information",
          "parentId": "d16f0af9-4657-476a-b554-8ecef84fbee9",
          "collapsed": false,
          "unread": true,
          "bookmarked": false,
          "searchResultState": null
        }
      },
      "generating": null
    }
  }
}